{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "00051b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFindings:\\n    Methods:\\n    1. BOG unigram\\n    2. BOG bigram\\n    3. TF-IDF\\n    \\n    Overall, these methods create many features\\n    \\n    Possible solution: Dimentionality reduction\\n    1. BOG unigram using LatentDirichletAllocation\\n    \\n\\nTODO: \\n    - word2vec? We would have to train new weights based on corpus\\n    \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Findings:\n",
    "    Methods:\n",
    "    1. BOG unigram\n",
    "    2. BOG bigram\n",
    "    3. TF-IDF\n",
    "    \n",
    "    Overall, these methods create too many features\n",
    "    \n",
    "    Possible solution: Dimentionality reduction\n",
    "    1. BOG unigram using LatentDirichletAllocation\n",
    "    2. BOG unigram using TruncatedSVD\n",
    "    \n",
    "    TODO: \n",
    "    1. Different dimensionality reduction methods\n",
    "    2. Cannot drop stop words since they are hashed. But we can drop K common words.\n",
    "    3. Try bigram or TF-IDF with dimensionality reduction.\n",
    "    4. Try feature selection rather than dimensionality reduction. (option: SGDClassifier)\n",
    "    5. Any paramater tuning.\n",
    "\n",
    "TODO: \n",
    "    - word2vec? We would have to train new weights based on corpus\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2f1905ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3b37f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV to Dataframe\n",
    "PATH = '../data/'\n",
    "FILE_TRAIN = 'XYtr.csv'\n",
    "FILE_TEST = 'Xte.csv'\n",
    "df_train = pd.read_csv(PATH + FILE_TRAIN)\n",
    "df_test = pd.read_csv(PATH + FILE_TEST)\n",
    "# Fill empty descriptions with a unique 'empty' token.\n",
    "description_train = df_train['description']\n",
    "description_train = description_train.fillna('NAN')\n",
    "description_test = df_test['description']\n",
    "description_test = description_test.fillna('NAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9f15b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6914, 14381)\n",
      "(6914, 14381)\n"
     ]
    }
   ],
   "source": [
    "# Create the corpus using the training and test data\n",
    "corpus = list(description_train)+list(description_test)\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = vectorizer.fit_transform(corpus)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create the features for the seperate train and test sets\n",
    "sequences = list(description_train)\n",
    "sequences_train = vectorizer.transform(sequences).toarray()\n",
    "word_features_train = pd.DataFrame(sequences_train, columns=features)\n",
    "print(word_features_train.shape)\n",
    "\n",
    "sequences = list(description_test)\n",
    "sequences_test = vectorizer.transform(sequences).toarray()\n",
    "word_features_test = pd.DataFrame(sequences_test, columns=features)\n",
    "print(word_features_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5ed81884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['002n7 5o5hz' '002n7 gq5ct' '002n7 rkasv' ... 'zzw3j lbjbk' 'zzw3j zbbrf'\n",
      " 'zzymt cdzuk']\n",
      "(6914, 78508)\n"
     ]
    }
   ],
   "source": [
    "# Using bigrams (creates many more features). (2, 2) means ONLY bigrams\n",
    "corpus = list(description_train)+list(description_test)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(features)\n",
    "\n",
    "# Create the features for the seperate train and test sets\n",
    "sequences = list(description_train)\n",
    "sequences_train = vectorizer.transform(sequences).toarray()\n",
    "word_features_train = pd.DataFrame(sequences_train, columns=features)\n",
    "print(word_features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a9836474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6914, 14381)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# Term Frequency: This summarizes how often a given word appears within a document.\n",
    "# Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "corpus = list(description_train)+list(description_test)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "sequences = list(description_train)\n",
    "sequences_train = vectorizer.transform(sequences).toarray()\n",
    "word_features_train = pd.DataFrame(sequences_train, columns=features)\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "86fd2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13828, 10)\n"
     ]
    }
   ],
   "source": [
    "# LDA from lecture (dimensionality reduction)\n",
    "K = 10\n",
    "corpus = list(description_train)+list(description_test)\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = vectorizer.fit_transform(corpus)\n",
    "lda = LatentDirichletAllocation(n_components = K)\n",
    "lda.fit(corpus)\n",
    "topics = lda.transform(corpus)\n",
    "\n",
    "print(topics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "85356716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5762409908464238\n"
     ]
    }
   ],
   "source": [
    "# TruncatedSVD (dimensionality reduction)\n",
    "# Contrary to PCA, this estimator does not center the data before computing the singular value decomposition.\n",
    "K = 10\n",
    "corpus = list(description_train)+list(description_test)\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = vectorizer.fit_transform(corpus)\n",
    "\n",
    "svd = TruncatedSVD(n_components=K, n_iter=7, random_state=42)\n",
    "svd.fit(corpus)\n",
    "\n",
    "# Total variance explained.\n",
    "print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "15d8caf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRough work\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Rough work\n",
    "'''\n",
    "# # Create the corpus and bag of words.\n",
    "# corpus = description.values.tolist()\n",
    "# vectorizer = CountVectorizer()\n",
    "# vectorizer.fit(corpus)\n",
    "# # Each word is a an index of a vector of size .\n",
    "# # print(vectorizer.vocabulary_)\n",
    "\n",
    "# #Create the bag of words.\n",
    "# bag_of_words = vectorizer.transform(corpus)\n",
    "# # (sequence number, index assigned to word from fit) -> count\n",
    "# # print(bag_of_words)\n",
    "\n",
    "# # We can see that for sequence 0, the index 627 (representing word 627) is count 1\n",
    "# # print('bag of words as an array:\\n{}'.format(bag_of_words.toarray()[0][627]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e22f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
