{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1467d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b672d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backtest:\n",
    "    \"\"\"\n",
    "    Runs backtesting on the provided data for a supplied model and provides help er methods.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.data = pd.read_csv('BC_COVID_CASES_Cases.csv')\n",
    "        self.wastewater_data = pd.read_csv('BC_COVID_CASES_Wastewater.csv')\n",
    "        self.join_wastewater_data()\n",
    "        self.clean_data()\n",
    "        self.predictions_info = pd.read_csv('predictions.csv')['Date:Delay'].to_list()\n",
    "        self.predictions = pd.read_csv('predictions.csv')\n",
    "        self.predictions['Actual'] = 0 #For calculating RMSPE\n",
    "    \n",
    "\n",
    "    def __call__(self, output_predication_csv=False, alpha=None, poly=2, hyper_params=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Backtests model for every prediction.\n",
    "        No forward contamination\n",
    "        Method: \n",
    "        1. Shift the target 'New cases' up T - D days\n",
    "        2. Get the T - D day and create X_prime\n",
    "        3. From the original data test against T\n",
    "        \"\"\"\n",
    "        # Manage arguments\n",
    "        try:\n",
    "            model = kwargs['model']\n",
    "            days = kwargs['days']\n",
    "            features = kwargs['features']\n",
    "        except KeyError:\n",
    "            print('Required argument is missing')\n",
    "            return None\n",
    "\n",
    "        for items in self.predictions_info:\n",
    "            items = items.split(':')\n",
    "            T = items[0]\n",
    "            D = int(items[1])\n",
    "\n",
    "            X_prime, idx = self.construct_x_prime(T, D)\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            X_train = X_prime.loc[:, features]\n",
    "            X_test = self.data.loc[idx+1, features]\n",
    "            Y_train = X_prime['New cases']\n",
    "            Y_test = self.data.loc[idx+D+1, 'New cases']\n",
    "\n",
    "\n",
    "              ########################################\n",
    "              # if T == '2020-04-01' and D == 1:\n",
    "              #   print(X_train)\n",
    "              #   print(Y_train)\n",
    "              #   print(X_test)\n",
    "              #   print(Y_test)\n",
    "              #   break\n",
    "              ########################################\n",
    "\n",
    "            Y_hat = None\n",
    "            if model == 'regression':\n",
    "                Y_hat = self.run_regression(X_train, Y_train, X_test, days, model=LinearRegression())\n",
    "            elif model == 'ridge_regression':\n",
    "                Y_hat = self.run_regression(X_train, Y_train, X_test, days, model=Ridge(alpha=alpha))\n",
    "            elif model == 'lasso_regression':\n",
    "                Y_hat = self.run_regression(X_train, Y_train, X_test, days, model=Lasso(alpha=alpha, tol=0.01))\n",
    "            elif model == 'polynomial_regression':\n",
    "                Y_hat = self.run_polynomial_regression(X_train, Y_train, X_test, days, poly)\n",
    "            elif model == 'spline':\n",
    "                Y_hat = self.run_spline(X_train, Y_train, X_test, days)\n",
    "            elif model == 'xg_boost':\n",
    "                Y_hat = self.run_xg_boost(X_train, Y_train, X_test, hyper_params)\n",
    "            elif model == 'svm':\n",
    "                Y_hat = self.run_svm(X_train, Y_train, X_test, days)\n",
    "            elif model =='linsvm':\n",
    "                Y_hat = self.run_linsvm(X_train, Y_train, X_test,days)\n",
    "            else:\n",
    "                print('Please pass a model to train and evaluate.')\n",
    "\n",
    "            self.add_prediction(Y_hat, Y_test, T, D)\n",
    "\n",
    "        RMSPE = self.RMSPE()\n",
    "        if output_predication_csv: self.output_csv(model)\n",
    "        return RMSPE\n",
    "\n",
    "    def construct_x_prime(self, T, D):\n",
    "        \"\"\"\n",
    "        Helper method. Returns X_prime and the index for the last row of X_prime.\n",
    "        \"\"\"\n",
    "        T_prime = (datetime.strptime(T, '%Y-%m-%d') - timedelta(D+1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        # Shift the target 'New cases' up by D days\n",
    "        X_prime = self.data.copy() \n",
    "        X_prime['New cases'] = X_prime['New cases'].shift(-D)\n",
    "\n",
    "        # idx is the index of the last row of X'\n",
    "        idx = X_prime.index[X_prime['Date'] == T_prime].tolist()[0]\n",
    "        X_prime = X_prime[0:idx+1]\n",
    "        return X_prime, idx\n",
    "\n",
    "    def train_test_sets(self, X, num = 4) -> list:\n",
    "        \"\"\"\n",
    "        Helper function. Returns a list of tuples containg the data frame X split num ways\n",
    "        Output: [(X_train, X_test) ...]\n",
    "        \"\"\"\n",
    "        TrainTest = namedtuple('TrainTest', 'train test')\n",
    "        output = list()\n",
    "        kf = KFold(n_splits = num)\n",
    "        kf.get_n_splits(X)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train = X.iloc[train_index]\n",
    "            X_test = X.iloc[test_index]\n",
    "            output.append(TrainTest(X_train, X_test))\n",
    "        return output\n",
    "\n",
    "    def RMSPE(self) -> float:\n",
    "        \"\"\"\n",
    "        Root Mean Squared Predication Error\n",
    "        \"\"\"\n",
    "        # Count => Y_hat, Actual => Y\n",
    "        return np.sqrt(np.mean((self.predictions['Count'] - self.predictions['Actual'])**2))\n",
    "  \n",
    "    def join_wastewater_data(self):\n",
    "        self.wastewater_data['Date']= pd.to_datetime(self.wastewater_data['Date'], dayfirst=True)\n",
    "        self.wastewater_data = self.wastewater_data.pivot(index='Date', columns='Plant')\n",
    "        self.wastewater_data.columns = self.wastewater_data.columns.droplevel(0)\n",
    "        self.wastewater_data['Date'] = self.wastewater_data.index.astype(str)\n",
    "        self.wastewater_data = self.wastewater_data.rename_axis(None)\n",
    "        self.wastewater_data = self.wastewater_data.fillna(value=0)\n",
    "        self.data = self.data.merge(self.wastewater_data, on='Date', how='left')\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        Cleans the given data set.\n",
    "        Methodology:\n",
    "        1. For the wastewater data, 0 seems to be equivilent to NaN\n",
    "        2. Forward fill all values\n",
    "        3. Only NaN is left, so set them to 0\n",
    "        4. Try changing cummulative features to daily, set adjustments to zero\n",
    "        \"\"\"\n",
    "        # cummulative = [\n",
    "        #   'Cumulative cases',\n",
    "        #   'Cumulative Vancouver Coastal',\n",
    "        #   'Cumulative Fraser Health',\n",
    "        #   'Cumulative Island Health ',\n",
    "        #   'Cumulative Interior Health',\n",
    "        #   'Cumulative Northern Health',\n",
    "        #   'Recovered'\n",
    "        # ]\n",
    "\n",
    "        # self.data.loc[:, 'Annacis Island':'Northwest Langley'] = self.data.loc[:, 'Annacis Island':'Northwest Langley'].replace(0, np.nan)\n",
    "        # self.data[cummulative] = self.data[cummulative].diff()\n",
    "        # self.data.loc[0,'Cumulative cases':'Cumulative Vancouver Coastal'] = 1\n",
    "        # self.data = self.data.fillna(method='ffill')\n",
    "        # self.data = self.data.fillna(value=0)\n",
    "\n",
    "        # # Clear the last row from fill forward\n",
    "        # self.data.loc[self.data.shape[0]-1,'New cases':] = np.nan\n",
    "\n",
    "        # # There are small adjustments/corrections in the cummulative data.\n",
    "        # numbers = self.data._get_numeric_data()\n",
    "        # numbers[numbers < 0] = 0\n",
    "\n",
    "        self.data.loc[:, 'Annacis Island':'Northwest Langley'] = self.data.loc[:, 'Annacis Island':'Northwest Langley'].fillna(value=0)\n",
    "        self.data = self.data.fillna(method='ffill')\n",
    "        self.data = self.data.fillna(value=0)\n",
    "    \n",
    "    def add_prediction(self, Y_hat, Y_test, T, D):\n",
    "        \"\"\"\n",
    "        Adds the predication of the model to the predications output.\n",
    "        \"\"\"\n",
    "        # Get the index of the Date:Delay signature\n",
    "        idx = self.predictions.index[self.predictions['Date:Delay'] == f'{T}:{D}'].tolist()[0]\n",
    "        self.predictions.loc[idx,'Count'] = Y_hat\n",
    "        self.predictions.loc[idx,'Actual'] = Y_test\n",
    "  \n",
    "    def get_sample_data(self, signature):\n",
    "            \"\"\"\n",
    "            Returns the X' dataset based on the signature 'YYYY-MM-DD:PD'.\n",
    "            Can be used for benchmarking, tuning, and implementing models.\n",
    "            \"\"\"\n",
    "            items = signature.split(':')\n",
    "            T = items[0]\n",
    "            D = int(items[1])\n",
    "            return self.construct_x_prime(T, D)[0]\n",
    "\n",
    "    def output_csv(self, model):\n",
    "        file = f'{model}_predictions.csv'\n",
    "        folder = './output'\n",
    "\n",
    "        if not os.path.exists(folder):\n",
    "          os.mkdir(folder)\n",
    "        full_path = os.path.join(folder, file)   \n",
    "\n",
    "        output_to_upload = self.predictions[['Date:Delay', 'Count']]\n",
    "        output_to_upload.to_csv(full_path, index=False)\n",
    "\n",
    "\n",
    "    \n",
    "    def run_linsvm(self, X_train, Y_train, X_test, days) -> float:\n",
    "        \"\"\"\n",
    "        Returns a prediction using linear support vector regression\n",
    "        \"\"\"\n",
    "        if days: \n",
    "            X_train = X_train.tail(days)\n",
    "            Y_train = Y_train.tail(days)\n",
    "    \n",
    "        X_train = X_train.to_numpy()\n",
    "        X_test = X_test.to_numpy().reshape(1, -1)\n",
    "        Y_train = Y_train.to_numpy()\n",
    "\n",
    "        #model = LinearSVR(C=1.0, dual=True, fit_intercept=True,intercept_scaling=1.0, max_iter=10000000,random_state=None, tol=0.0001, verbose=0)\n",
    "        model = make_pipeline(StandardScaler(), LinearSVR(random_state=0, tol=1e-5, max_iter=1000))\n",
    "        #Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                #('linearsvr', LinearSVR(random_state=0, tol=1e-05))])\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_hat = model.predict(X_test)\n",
    "        return Y_hat\n",
    "\n",
    "    def run_svm(self, X_train, Y_train, X_test, days) -> float:\n",
    "        \"\"\"\n",
    "        Returns a prediction using linear support vector regression\n",
    "        \"\"\"\n",
    "        if days: \n",
    "            X_train = X_train.tail(days)\n",
    "            Y_train = Y_train.tail(days)\n",
    "    \n",
    "        X_train = X_train.to_numpy()\n",
    "        X_test = X_test.to_numpy().reshape(1, -1)\n",
    "        Y_train = Y_train.to_numpy()\n",
    "\n",
    "        #model = LinearSVR(C=1.0, dual=True, fit_intercept=True,intercept_scaling=1.0, max_iter=10000000,random_state=None, tol=0.0001, verbose=0)\n",
    "        model = make_pipeline(StandardScaler(), SVR(tol=1e-5, max_iter=1000, C=15.0, epsilon=0.1))\n",
    "        #Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                #('linearsvr', LinearSVR(random_state=0, tol=1e-05))])\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_hat = model.predict(X_test)\n",
    "        return Y_hat\n",
    "\n",
    "\n",
    "    \n",
    "#     def run_spline(self, X_train, Y_train, X_test, days) -> float:\n",
    "#         if days: \n",
    "#             X_train = X_train.tail(days)\n",
    "#             Y_train = Y_train.tail(days)\n",
    "\n",
    "#         X_train = X_train.to_numpy()\n",
    "#         X_test = X_test.to_numpy().reshape(1, -1)\n",
    "\n",
    "#         model = make_pipeline(SplineTransformer(n_knots=5, degree=2), Ridge(alpha=1e-3))\n",
    "#         model.fit(X_train, Y_train)\n",
    "#         Y_hat = model.predict(X_test)[0]\n",
    "#         return Y_hat\n",
    "\n",
    "#     def run_xg_boost(self, X_train, Y_train, X_test, hyper_params) -> float:\n",
    "\n",
    "#         X_train = X_train.to_numpy()\n",
    "#         Y_train = Y_train.to_numpy()\n",
    "#         X_test = X_test.to_numpy().reshape(1, -1)\n",
    "\n",
    "#         model = XGBRegressor()\n",
    "#         # model = XGBRegressor(n_estimators=hyper_params.t, max_depth=hyper_params.d, eta=hyper_params.r, subsample=hyper_params.s)\n",
    "#         model.fit(X_train, Y_train)\n",
    "#         Y_hat = model.predict(X_test)[0].tolist()\n",
    "#         return Y_hat\n",
    "\n",
    "\n",
    "    features = [\n",
    "    #     'Cumulative cases',\n",
    "    #     'Cumulative Vancouver Coastal',\n",
    "        'Hospitalization (Currently in hospital)',\n",
    "        'ICU (Currently in ICU)',\n",
    "        'Recovered',\n",
    "        'Deaths',\n",
    "    #     'Active cases',\n",
    "    #     'Annacis Island'\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a90bf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.15956856965397\n"
     ]
    }
   ],
   "source": [
    "model = Backtest()\n",
    "\n",
    "features = [\n",
    "     'Cumulative cases',\n",
    "     'Cumulative Vancouver Coastal'\n",
    "]\n",
    "\n",
    "RMSPE = model(output_predication_csv=True, model='svm', days=7, features=features)\n",
    "#days=34\n",
    "print(RMSPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a78cd551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 1  RMSPE: 69.50420815645849\n",
      "day: 2  RMSPE: 70.80167688698769\n",
      "day: 3  RMSPE: 74.5216245832095\n",
      "day: 4  RMSPE: 75.25463239126427\n",
      "day: 5  RMSPE: 76.8489793902487\n",
      "day: 6  RMSPE: 73.36712076597212\n",
      "day: 7  RMSPE: 72.15956856965397\n",
      "day: 8  RMSPE: 72.71219234323196\n",
      "day: 9  RMSPE: 76.08715049346121\n",
      "day: 10  RMSPE: 79.03431747140317\n",
      "day: 11  RMSPE: 81.72552021674024\n",
      "day: 12  RMSPE: 83.72169686530975\n",
      "day: 13  RMSPE: 85.80265334192224\n",
      "day: 14  RMSPE: 87.04597247590702\n",
      "day: 15  RMSPE: 88.94680874462178\n",
      "day: 16  RMSPE: 91.5566922400323\n",
      "day: 17  RMSPE: 94.4330193715007\n",
      "day: 18  RMSPE: 97.0716391264759\n",
      "day: 19  RMSPE: 99.23208969669288\n",
      "day: 20  RMSPE: 101.11364497413132\n",
      "day: 21  RMSPE: 103.17070323215715\n",
      "day: 22  RMSPE: 104.81944220173209\n",
      "day: 23  RMSPE: 106.66717206228823\n",
      "day: 24  RMSPE: 108.81343354795297\n",
      "day: 25  RMSPE: 111.01422322946756\n",
      "day: 26  RMSPE: 112.95889432888787\n",
      "day: 27  RMSPE: 114.82770491351332\n",
      "day: 28  RMSPE: 116.50724624720122\n",
      "day: 29  RMSPE: 118.2452816888885\n",
      "day: 30  RMSPE: 120.08936732002269\n",
      "day: 31  RMSPE: 121.99327444261196\n",
      "day: 32  RMSPE: 123.78063081697044\n",
      "day: 33  RMSPE: 125.39711227945102\n"
     ]
    }
   ],
   "source": [
    "for day in range(1,34):\n",
    "    #for C in np.linspace(1,5,16):\n",
    "        RMSPE = model(output_predication_csv=False, model='svm', days=day, features=features)\n",
    "        print(f'day: {day}  RMSPE: {RMSPE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce252e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
