{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1b1b1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine.creation import CyclicalTransformer\n",
    "from matplotlib.dates import DateFormatter\n",
    "import os\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.color import rgba2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f85ad7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6914 entries, 0 to 6913\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           6914 non-null   object \n",
      " 1   X.sales      6914 non-null   int64  \n",
      " 2   cdate        6914 non-null   object \n",
      " 3   description  6512 non-null   object \n",
      " 4   version      6746 non-null   object \n",
      " 5   symbol       5555 non-null   object \n",
      " 6   ext          6914 non-null   object \n",
      " 7   fee1         6696 non-null   float64\n",
      " 8   fee2         6705 non-null   float64\n",
      " 9   total        6914 non-null   float64\n",
      "dtypes: float64(3), int64(1), object(6)\n",
      "memory usage: 540.3+ KB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6914 entries, 0 to 6913\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           6914 non-null   object \n",
      " 1   X.sales      6914 non-null   int64  \n",
      " 2   cdate        6914 non-null   object \n",
      " 3   description  6537 non-null   object \n",
      " 4   version      6760 non-null   object \n",
      " 5   symbol       5532 non-null   object \n",
      " 6   ext          6914 non-null   object \n",
      " 7   fee1         6630 non-null   float64\n",
      " 8   fee2         6638 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 486.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load CSV to Dataframe\n",
    "#PATH = 'data/'\n",
    "FILE_train = 'XYtr.csv'\n",
    "FILE_test = 'Xte.csv'\n",
    "\n",
    "#raw_train = pd.read_csv(PATH + FILE_train)\n",
    "#raw_test = pd.read_csv(PATH + FILE_test)\n",
    "raw_train = pd.read_csv(FILE_train)\n",
    "raw_test = pd.read_csv(FILE_test)\n",
    "\n",
    "# Description, version, symbol, fee1, and fee2 have missing values (NaN)\n",
    "print(raw_train.info())\n",
    "print()\n",
    "print(raw_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1be1d830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_train shape:  (6914, 10)\n",
      "raw_test shape:  (6914, 9)\n"
     ]
    }
   ],
   "source": [
    "print('raw_train shape: ', raw_train.shape)\n",
    "print('raw_test shape: ', raw_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "288eeb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6914 entries, 0 to 6913\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           6914 non-null   object \n",
      " 1   X.sales      6914 non-null   int64  \n",
      " 2   cdate        6914 non-null   object \n",
      " 3   description  6914 non-null   object \n",
      " 4   version      6914 non-null   object \n",
      " 5   symbol       6914 non-null   object \n",
      " 6   ext          6914 non-null   object \n",
      " 7   fee1         6914 non-null   float64\n",
      " 8   fee2         6914 non-null   float64\n",
      " 9   total        6914 non-null   float64\n",
      "dtypes: float64(3), int64(1), object(6)\n",
      "memory usage: 540.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_clean = raw_train.copy()\n",
    "test_clean = raw_test.copy()\n",
    "\n",
    "# description: use the token None to mean no description\n",
    "train_clean['description'] = train_clean['description'].fillna('None')\n",
    "\n",
    "# version: Has 'None' category. Set nan to 'None'. \n",
    "#print(train_train['version'].unique())\n",
    "train_clean['version'] = train_clean['version'].fillna('None')\n",
    "\n",
    "# symbol: 5 digit symbols. Set to 00000 to represent None.\n",
    "# print(df_train['symbol'].unique())\n",
    "train_clean['symbol'] = train_clean['symbol'].fillna('00000')\n",
    "\n",
    "\n",
    "# fee1: Small number misssin. Fill with the mean.\n",
    "#df_train['fee1'] = df_train['fee1'].fillna((df_train['fee1'].mean()))\n",
    "# https://www.w3resource.com/python-exercises/pandas/missing-values/python-pandas-missing-values-exercise-14.php\n",
    "train_clean['fee1'].fillna(train_clean['fee1'].median(), inplace=True)\n",
    "                                           \n",
    "# fee2: Small number misssin. Fill with the mean.\n",
    "#df_train['fee2'] = df_train['fee2'].fillna((df_train['fee2'].mean()))\n",
    "train_clean['fee2'].fillna(train_clean['fee2'].median(), inplace=True)\n",
    "\n",
    "\n",
    "print(train_clean.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6ef6852d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6914 entries, 0 to 6913\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           6914 non-null   object \n",
      " 1   X.sales      6914 non-null   int64  \n",
      " 2   cdate        6914 non-null   object \n",
      " 3   description  6914 non-null   object \n",
      " 4   version      6914 non-null   object \n",
      " 5   symbol       6914 non-null   object \n",
      " 6   ext          6914 non-null   object \n",
      " 7   fee1         6914 non-null   float64\n",
      " 8   fee2         6914 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 486.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# description: use the token None to mean no description\n",
    "test_clean['description'] = test_clean['description'].fillna('None')\n",
    "\n",
    "# version: Has 'None' category. Set nan to 'None'. \n",
    "test_clean['version'] = test_clean['version'].fillna('None')\n",
    "\n",
    "# symbol: 5 digit symbols. Set to 00000 to represent None.\n",
    "test_clean['symbol'] = test_clean['symbol'].fillna('00000')\n",
    "\n",
    "# fee1: Small number misssin. Fill with the mean.\n",
    "test_clean['fee1'].fillna(test_clean['fee1'].median(), inplace=True)\n",
    "                                           \n",
    "# fee2: Small number misssin. Fill with the mean.\n",
    "test_clean['fee2'].fillna(test_clean['fee2'].median(), inplace=True)\n",
    "\n",
    "\n",
    "print(test_clean.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "eded09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = train_clean.copy()\n",
    "test_processed = test_clean.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3bc4a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdate: change dates to float\n",
    "tr_date = train_processed['cdate']\n",
    "train_processed['cdate'] = pd.to_datetime(tr_date).values.astype(np.float64)/8.64e+13\n",
    "\n",
    "te_date = test_processed['cdate']\n",
    "test_processed['cdate'] = pd.to_datetime(te_date).values.astype(np.float64)/8.64e+13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "214169e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.sales: change from int to float\n",
    "train_processed['X.sales'] = train_processed['X.sales'].astype(np.float64)\n",
    "test_processed['X.sales'] = test_processed['X.sales'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f3bc326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain unique values from each dataset\n",
    "train_symbols = set(train_processed['symbol'].unique())\n",
    "test_symbols = set(test_processed['symbol'].unique())\n",
    "\n",
    "# union all the values\n",
    "# https://stackoverflow.com/questions/52976664/python-differences-between-two-lists\n",
    "# # https://www.programiz.com/python-programming/methods/set/union\n",
    "all_symbols = train_symbols.union(test_symbols)\n",
    "\n",
    "# values not included in train set\n",
    "train_required_symbols = list(all_symbols - train_symbols)\n",
    "\n",
    "# values not included in test set\n",
    "test_required_symbols = list(all_symbols - test_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0a7a72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding on version, symbol and ext\n",
    "train_processed = pd.get_dummies(train_processed, columns = ['version', 'ext', 'symbol'], drop_first = False, prefix = ['version', 'ext', 'symbol'])\n",
    "test_processed = pd.get_dummies(test_processed, columns = ['version', 'ext', 'symbol'], drop_first = False, prefix = ['version', 'ext', 'symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0b095041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3364: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/18674064/how-do-i-insert-a-column-at-a-specific-column-index-in-pandas\n",
    "for train_syms in train_required_symbols:\n",
    "    train_processed.insert(train_processed.shape[1], str('symbol_') + train_syms, 0)\n",
    "train_base = train_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b81ae6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_syms in test_required_symbols:\n",
    "    test_processed.insert(test_processed.shape[1], str('symbol_') + test_syms, 0)\n",
    "test_base = test_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0c94cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base = train_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "824b00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_base = test_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0396d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train description missing values:  0\n",
      "test description missing values:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check for missing values before processing\n",
    "print('train description missing values: ', train_base['description'].isnull().sum())\n",
    "print('test description missing values: ', test_base['description'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1c6081d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus using training and test data\n",
    "corpus = list(train_base['description'])+list(test_base['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c1837797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vectorizer Object\n",
    "# remove tokens that appear in 10% of the documents\n",
    "# remove unique tokens that appear in, at most, 2 documents\n",
    "vectorizer = CountVectorizer(max_df=0.1, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "64648b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode the documents in a count matrix\n",
    "corpus_vectorized = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fc38d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature name:  ['002n7' '00b0d' '00jhg' ... 'zztgg' 'zzvdf' 'zzw3j']\n",
      "feature size:  10566\n",
      "matrix dimension:  (13828, 10566)\n"
     ]
    }
   ],
   "source": [
    "# Feature names and size\n",
    "print('feature name: ', vectorizer.get_feature_names_out())\n",
    "print('feature size: ', len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "# dimension of a sparse matrix of documents (row) vs number of unique words\n",
    "print('matrix dimension: ', corpus_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d0c5ad8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>002n7</th>\n",
       "      <th>00b0d</th>\n",
       "      <th>00jhg</th>\n",
       "      <th>00ud9</th>\n",
       "      <th>00xck</th>\n",
       "      <th>01abs</th>\n",
       "      <th>01fnu</th>\n",
       "      <th>01jsj</th>\n",
       "      <th>01k0e</th>\n",
       "      <th>01nrz</th>\n",
       "      <th>...</th>\n",
       "      <th>zzhb3</th>\n",
       "      <th>zzht0</th>\n",
       "      <th>zzlz3</th>\n",
       "      <th>zznp1</th>\n",
       "      <th>zzns7</th>\n",
       "      <th>zzpvk</th>\n",
       "      <th>zzr1c</th>\n",
       "      <th>zztgg</th>\n",
       "      <th>zzvdf</th>\n",
       "      <th>zzw3j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13823</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13825</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13826</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13827</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13828 rows × 10566 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       002n7  00b0d  00jhg  00ud9  00xck  01abs  01fnu  01jsj  01k0e  01nrz  \\\n",
       "0          0      0      0      0      0      0      0      0      0      0   \n",
       "1          0      0      0      0      0      0      0      0      0      0   \n",
       "2          0      0      0      0      0      0      0      0      0      0   \n",
       "3          0      0      0      0      0      0      0      0      0      0   \n",
       "4          0      0      0      0      0      0      0      0      0      0   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "13823      0      0      0      0      0      0      0      0      0      0   \n",
       "13824      0      0      0      0      0      0      0      0      0      0   \n",
       "13825      0      0      0      0      0      0      0      0      0      0   \n",
       "13826      0      0      0      0      0      0      0      0      0      0   \n",
       "13827      0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "       ...  zzhb3  zzht0  zzlz3  zznp1  zzns7  zzpvk  zzr1c  zztgg  zzvdf  \\\n",
       "0      ...      0      0      0      0      0      0      0      0      0   \n",
       "1      ...      0      0      0      0      0      0      0      0      0   \n",
       "2      ...      0      0      0      0      0      0      0      0      0   \n",
       "3      ...      0      0      0      0      0      0      0      0      0   \n",
       "4      ...      0      0      0      0      0      0      0      0      0   \n",
       "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "13823  ...      0      0      0      0      0      0      0      0      0   \n",
       "13824  ...      0      0      0      0      0      0      0      0      0   \n",
       "13825  ...      0      0      0      0      0      0      0      0      0   \n",
       "13826  ...      0      0      0      0      0      0      0      0      0   \n",
       "13827  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "       zzw3j  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "13823      0  \n",
       "13824      0  \n",
       "13825      0  \n",
       "13826      0  \n",
       "13827      0  \n",
       "\n",
       "[13828 rows x 10566 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A sparse matrix of documents (row) vs number of unique words\n",
    "count_array = corpus_vectorized.toarray()\n",
    "corpus_df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ad88e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model = LatentDirichletAllocation(n_components = 10,\n",
    "                                     learning_method = 'online',\n",
    "                                      learning_decay = 0.9,\n",
    "                                      random_state=100, \n",
    "                                      batch_size=128, \n",
    "                                      evaluate_every = -1, \n",
    "                                      n_jobs = -1)\n",
    "\n",
    "lda_output = lda_model.fit_transform(corpus_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c370556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13828, 10)\n",
      "LatentDirichletAllocation(learning_decay=0.9, learning_method='online',\n",
      "                          n_jobs=-1, random_state=100)\n"
     ]
    }
   ],
   "source": [
    "print(lda_output.shape)\n",
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3eafea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\n",
    "train_document_topic = pd.DataFrame(np.round(lda_output[0:6914,], 8), columns = topicnames)\n",
    "test_document_topic = pd.DataFrame(np.round(lda_output[6914:13828,], 8), columns = topicnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0cd37183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005557</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005558</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.949996</td>\n",
       "      <td>0.005556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.849981</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.096422</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.870244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>0.054822</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.159625</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.771827</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.699989</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033344</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6911</th>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.003125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6912</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.909998</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6914 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic0    Topic1    Topic2    Topic3    Topic4    Topic5    Topic6  \\\n",
       "0     0.005557  0.005556  0.005558  0.005556  0.005556  0.005556  0.005556   \n",
       "1     0.016667  0.016667  0.016667  0.016685  0.016667  0.016667  0.016667   \n",
       "2     0.700000  0.033333  0.033333  0.033333  0.033333  0.033333  0.033333   \n",
       "3     0.004167  0.004167  0.004167  0.004167  0.004167  0.004167  0.004167   \n",
       "4     0.050000  0.050000  0.050000  0.550000  0.050000  0.050000  0.050000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6909  0.054822  0.001961  0.001961  0.001961  0.001961  0.159625  0.001961   \n",
       "6910  0.033333  0.699989  0.033333  0.033333  0.033344  0.033333  0.033333   \n",
       "6911  0.003125  0.003125  0.003125  0.003125  0.003125  0.003125  0.003125   \n",
       "6912  0.010000  0.010000  0.010000  0.010000  0.010000  0.010000  0.010000   \n",
       "6913  0.050000  0.050000  0.050000  0.550000  0.050000  0.050000  0.050000   \n",
       "\n",
       "        Topic7    Topic8    Topic9  \n",
       "0     0.005556  0.949996  0.005556  \n",
       "1     0.016667  0.849981  0.016667  \n",
       "2     0.033333  0.033333  0.033333  \n",
       "3     0.096422  0.004167  0.870244  \n",
       "4     0.050000  0.050000  0.050000  \n",
       "...        ...       ...       ...  \n",
       "6909  0.771827  0.001961  0.001961  \n",
       "6910  0.033333  0.033333  0.033333  \n",
       "6911  0.003125  0.971875  0.003125  \n",
       "6912  0.010000  0.909998  0.010000  \n",
       "6913  0.050000  0.050000  0.050000  \n",
       "\n",
       "[6914 rows x 10 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_document_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "761d58ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.918179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.188026</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.007696</td>\n",
       "      <td>0.007697</td>\n",
       "      <td>0.229133</td>\n",
       "      <td>0.368083</td>\n",
       "      <td>0.168595</td>\n",
       "      <td>0.007692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.558962</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.157168</td>\n",
       "      <td>0.226540</td>\n",
       "      <td>0.037973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.887499</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012501</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.989024</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>0.088165</td>\n",
       "      <td>0.093327</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.154617</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.648506</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6911</th>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.274999</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6912</th>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.957142</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.004762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6914 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic0    Topic1    Topic2    Topic3    Topic4    Topic5    Topic6  \\\n",
       "0     0.009091  0.009092  0.009091  0.009091  0.009091  0.009091  0.009091   \n",
       "1     0.050000  0.050000  0.050000  0.550000  0.050000  0.050000  0.050000   \n",
       "2     0.188026  0.007692  0.007693  0.007694  0.007696  0.007697  0.229133   \n",
       "3     0.558962  0.003226  0.003226  0.003226  0.003226  0.003226  0.003226   \n",
       "4     0.012500  0.012500  0.012500  0.012500  0.887499  0.012500  0.012500   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6909  0.001220  0.001220  0.001220  0.001220  0.001220  0.001220  0.989024   \n",
       "6910  0.088165  0.093327  0.002564  0.154617  0.002564  0.002564  0.002564   \n",
       "6911  0.025001  0.025000  0.025000  0.025000  0.525000  0.025000  0.274999   \n",
       "6912  0.004762  0.004762  0.004762  0.004762  0.957142  0.004762  0.004762   \n",
       "6913  0.700000  0.033333  0.033333  0.033333  0.033333  0.033333  0.033333   \n",
       "\n",
       "        Topic7    Topic8    Topic9  \n",
       "0     0.009091  0.009091  0.918179  \n",
       "1     0.050000  0.050000  0.050000  \n",
       "2     0.368083  0.168595  0.007692  \n",
       "3     0.157168  0.226540  0.037973  \n",
       "4     0.012501  0.012500  0.012500  \n",
       "...        ...       ...       ...  \n",
       "6909  0.001220  0.001220  0.001220  \n",
       "6910  0.648506  0.002564  0.002564  \n",
       "6911  0.025000  0.025000  0.025000  \n",
       "6912  0.004762  0.004762  0.004762  \n",
       "6913  0.033333  0.033333  0.033333  \n",
       "\n",
       "[6914 rows x 10 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_document_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER ='/Users/Jamie/venv/images/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "197b28eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>channels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8866.000000</td>\n",
       "      <td>8866.000000</td>\n",
       "      <td>8866.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26.243515</td>\n",
       "      <td>25.557861</td>\n",
       "      <td>3.597000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>205.798354</td>\n",
       "      <td>203.909154</td>\n",
       "      <td>0.975335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7200.000000</td>\n",
       "      <td>6810.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            height        width     channels\n",
       "count  8866.000000  8866.000000  8866.000000\n",
       "mean     26.243515    25.557861     3.597000\n",
       "std     205.798354   203.909154     0.975335\n",
       "min       4.000000    15.000000     0.000000\n",
       "25%      15.000000    15.000000     4.000000\n",
       "50%      15.000000    15.000000     4.000000\n",
       "75%      15.000000    15.000000     4.000000\n",
       "max    7200.000000  6810.000000     4.000000"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Image Processing: get list of filenames and read images into data-frame\n",
    "\n",
    "\n",
    "images = os.listdir(IMAGE_FOLDER)\n",
    "image_sizes = []\n",
    "file_names = []\n",
    "for image_file in images:\n",
    "    file_names.append(image_file[:-4])\n",
    "    image = imread('{}/{}'.format(IMAGE_FOLDER, image_file))\n",
    "    if len(image.shape) == 2:\n",
    "        height, width  = image.shape\n",
    "        shape = (height, width, 0)\n",
    "    else:\n",
    "        shape = image.shape\n",
    "    height, width, channels = shape\n",
    "    image_sizes.append([height, width, channels])\n",
    "\n",
    "image_df = pd.DataFrame(image_sizes, columns=['height', 'width', 'channels'])\n",
    "image_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9a86cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      height  width  channels                                id\n",
      "0         15     15         4  441867138fde60f8f6383a7473495bb7\n",
      "1         15     15         3  fbc1760fa9559aa945d876bcb844846c\n",
      "2         15     15         4  9296d9683e05e247964067ba474624b8\n",
      "3         14     15         4  d6cad9b97497746f6fd54cd9f44a72ca\n",
      "4         15     15         4  ecce7b6dd7d9b56ef615ba1256092c71\n",
      "...      ...    ...       ...                               ...\n",
      "8861      11     15         3  91bac11c1a918624d5f003a072e97e94\n",
      "8862      15     15         4  20e5e2cc3b755786e38f95ddef786e80\n",
      "8863      21     15         4  13e2d6ede635ba52651a79c682b7947e\n",
      "8864      15     15         4  a94c79f8278f537f678976a12853f789\n",
      "8865      11     15         4  b6a73ed4dd51c2b278d2be045b36f76b\n",
      "\n",
      "[8866 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "image_df['id'] = file_names\n",
    "print(image_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f69d7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds an indicator variable for greyscale images\n",
    "#https://stackoverflow.com/questions/26886653/pandas-create-new-column-based-on-values-from-other-columns-apply-a-function-o\n",
    "\n",
    "def greyscale (row):\n",
    "    if row['channels'] == 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "#image_df.apply (lambda row: greyscale(row), axis=1)\n",
    "image_df['greyscale'] = image_df.apply (lambda row: greyscale(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "017828fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds an indicator variable for large images\n",
    "def large_image (row):\n",
    "    if (row['height'] > 200)|(row['width'] > 200):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "#image_df.apply (lambda row: large_image(row), axis=1)\n",
    "image_df['large_image'] = image_df.apply (lambda row: large_image(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "44dd5adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 15, 3)\n",
      "(14, 15, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81de48d340>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image = 'ffc14dfa2038d68cf4f67ad39906addd.gif'\n",
    "image = imread('{}/{}'.format(IMAGE_FOLDER, sample_image))\n",
    "image = rgba2rgb(image)\n",
    "print(image.shape)\n",
    "testimage = resize(image, (15, 15))\n",
    "print(image.shape)\n",
    "\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a35737cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36078431]\n",
      " [0.39738562]\n",
      " [0.45751634]\n",
      " [0.46535948]\n",
      " [0.36339869]\n",
      " [0.30065359]\n",
      " [0.25228758]\n",
      " [0.26405229]\n",
      " [0.26143791]\n",
      " [0.24052288]\n",
      " [0.24052288]\n",
      " [0.22352941]\n",
      " [0.23267974]\n",
      " [0.4130719 ]\n",
      " [0.43006536]\n",
      " [0.3503268 ]\n",
      " [0.41568627]\n",
      " [0.5254902 ]\n",
      " [0.47843137]\n",
      " [0.34640523]\n",
      " [0.29673203]\n",
      " [0.2627451 ]\n",
      " [0.28104575]\n",
      " [0.24836601]\n",
      " [0.24313725]\n",
      " [0.26405229]\n",
      " [0.26405229]\n",
      " [0.27581699]\n",
      " [0.41830065]\n",
      " [0.43006536]\n",
      " [0.28496732]\n",
      " [0.40784314]\n",
      " [0.49803922]\n",
      " [0.38431373]\n",
      " [0.33856209]\n",
      " [0.27320261]\n",
      " [0.29281046]\n",
      " [0.32156863]\n",
      " [0.29934641]\n",
      " [0.29934641]\n",
      " [0.30457516]\n",
      " [0.27712418]\n",
      " [0.31372549]\n",
      " [0.4130719 ]\n",
      " [0.43006536]\n",
      " [0.16601307]\n",
      " [0.24705882]\n",
      " [0.21830065]\n",
      " [0.22091503]\n",
      " [0.36993464]\n",
      " [0.26666667]\n",
      " [0.21176471]\n",
      " [0.24705882]\n",
      " [0.26143791]\n",
      " [0.26013072]\n",
      " [0.22222222]\n",
      " [0.16862745]\n",
      " [0.17647059]\n",
      " [0.26405229]\n",
      " [0.34248366]\n",
      " [0.3124183 ]\n",
      " [0.25882353]\n",
      " [0.29019608]\n",
      " [0.29281046]\n",
      " [0.27189542]\n",
      " [0.24183007]\n",
      " [0.22614379]\n",
      " [0.22222222]\n",
      " [0.24705882]\n",
      " [0.23529412]\n",
      " [0.20522876]\n",
      " [0.15555556]\n",
      " [0.10849673]\n",
      " [0.14248366]\n",
      " [0.15163399]\n",
      " [0.48104575]\n",
      " [0.33333333]\n",
      " [0.37124183]\n",
      " [0.32156863]\n",
      " [0.1869281 ]\n",
      " [0.21045752]\n",
      " [0.24183007]\n",
      " [0.21176471]\n",
      " [0.23006536]\n",
      " [0.19477124]\n",
      " [0.1869281 ]\n",
      " [0.15294118]\n",
      " [0.13986928]\n",
      " [0.2745098 ]\n",
      " [0.20653595]\n",
      " [0.44575163]\n",
      " [0.34248366]\n",
      " [0.36078431]\n",
      " [0.27712418]\n",
      " [0.18562092]\n",
      " [0.2       ]\n",
      " [0.29019608]\n",
      " [0.25098039]\n",
      " [0.18431373]\n",
      " [0.18300654]\n",
      " [0.23137255]\n",
      " [0.21699346]\n",
      " [0.28496732]\n",
      " [0.4130719 ]\n",
      " [0.41830065]\n",
      " [0.3254902 ]\n",
      " [0.30196078]\n",
      " [0.33333333]\n",
      " [0.20522876]\n",
      " [0.14379085]\n",
      " [0.16339869]\n",
      " [0.35294118]\n",
      " [0.32941176]\n",
      " [0.14509804]\n",
      " [0.23137255]\n",
      " [0.34640523]\n",
      " [0.29019608]\n",
      " [0.36339869]\n",
      " [0.40261438]\n",
      " [0.41830065]\n",
      " [0.30588235]\n",
      " [0.2745098 ]\n",
      " [0.30588235]\n",
      " [0.29411765]\n",
      " [0.27843137]\n",
      " [0.28104575]\n",
      " [0.35947712]\n",
      " [0.35686275]\n",
      " [0.28366013]\n",
      " [0.32810458]\n",
      " [0.35686275]\n",
      " [0.31895425]\n",
      " [0.36601307]\n",
      " [0.40915033]\n",
      " [0.44183007]\n",
      " [0.32418301]\n",
      " [0.34771242]\n",
      " [0.23398693]\n",
      " [0.25882353]\n",
      " [0.34117647]\n",
      " [0.36470588]\n",
      " [0.32679739]\n",
      " [0.26797386]\n",
      " [0.28235294]\n",
      " [0.26797386]\n",
      " [0.28888889]\n",
      " [0.31764706]\n",
      " [0.36339869]\n",
      " [0.41568627]\n",
      " [0.43660131]\n",
      " [0.38300654]\n",
      " [0.49411765]\n",
      " [0.35163399]\n",
      " [0.21960784]\n",
      " [0.21960784]\n",
      " [0.29542484]\n",
      " [0.27843137]\n",
      " [0.19346405]\n",
      " [0.18562092]\n",
      " [0.18562092]\n",
      " [0.21176471]\n",
      " [0.29019608]\n",
      " [0.35555556]\n",
      " [0.34509804]\n",
      " [0.29411765]\n",
      " [0.28496732]\n",
      " [0.4627451 ]\n",
      " [0.49411765]\n",
      " [0.38300654]\n",
      " [0.23398693]\n",
      " [0.21699346]\n",
      " [0.23921569]\n",
      " [0.25751634]\n",
      " [0.26013072]\n",
      " [0.24183007]\n",
      " [0.26797386]\n",
      " [0.29542484]\n",
      " [0.26143791]\n",
      " [0.23267974]\n",
      " [0.20392157]\n",
      " [0.2496732 ]\n",
      " [0.40915033]\n",
      " [0.46013072]\n",
      " [0.49411765]\n",
      " [0.41699346]\n",
      " [0.26013072]\n",
      " [0.21960784]\n",
      " [0.23267974]\n",
      " [0.28888889]\n",
      " [0.30980392]\n",
      " [0.29150327]\n",
      " [0.24052288]\n",
      " [0.21045752]\n",
      " [0.19738562]\n",
      " [0.18039216]\n",
      " [0.30849673]\n",
      " [0.3869281 ]\n",
      " [0.43660131]\n",
      " [0.45751634]\n",
      " [0.50196078]\n",
      " [0.44444444]\n",
      " [0.26013072]\n",
      " [0.23921569]\n",
      " [0.23267974]\n",
      " [0.23137255]\n",
      " [0.23137255]\n",
      " [0.22745098]\n",
      " [0.21176471]\n",
      " [0.18431373]\n",
      " [0.16862745]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29500778089013385"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "FEATURE EXTRACTION: Mean RGB Value (unused)\n",
    "'''\n",
    "\n",
    "#height, width, channels = image.shape\n",
    "#mean_pixel_matrix = np.zeros((height, width))\n",
    "\n",
    "#for h in range(height):\n",
    "#    for w in range(width):\n",
    "#        mean_pixel_matrix[h][w] = (image[h][w][0] + image[h][w][1] + image[h][w][2])/channels\n",
    "\n",
    "#mean_pixel_matrix = mean_pixel_matrix.reshape(height * width, -1)\n",
    "#print(mean_pixel_matrix)\n",
    "#np.mean(mean_pixel_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4a8b075e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mq/qrk2b3yj28dcrt8fdwkh2p3r0000gn/T/ipykernel_7160/165886869.py:16: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  mean_pixel_matrix[h][w] = (image[h][w][0] + image[h][w][1] + image[h][w][2])/channels\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/qrk2b3yj28dcrt8fdwkh2p3r0000gn/T/ipykernel_7160/165886869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmean_pixel_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#adds the meanRGB value as a feature (unused)\n",
    "#meanRGB=[]\n",
    "\n",
    "#for image_file in images:\n",
    "#    image = imread('{}/{}'.format(IMAGE_FOLDER, image_file))\n",
    "#    if image.shape == (..., 4):\n",
    "        \n",
    "#        image = rgba2rgb(image)\n",
    "        \n",
    "    \n",
    "#    height, width, channels = image.shape\n",
    "#    mean_pixel_matrix = np.zeros((height, width))\n",
    "\n",
    "#    for h in range(height):\n",
    "#        for w in range(width):\n",
    "#            mean_pixel_matrix[h][w] = (image[h][w][0] + image[h][w][1] + image[h][w][2])/channels\n",
    "\n",
    "#    mean_pixel_matrix = mean_pixel_matrix.reshape(height * width, -1)\n",
    "#    mmpm=np.mean(mean_pixel_matrix)\n",
    "#    meanRGB.append([mmpm])\n",
    "   \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d73674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "FEATURE EXTRACTION: Edges (Sobel edge detection) (unused)\n",
    "'''\n",
    "\n",
    "# Find an image we can use that has more distinguishable edges\n",
    "#sample_image = 'fec874e713388b45ef26c4373aba16a5.jpg'\n",
    "#image = imread('{}/{}'.format(IMAGE_FOLDER, sample_image), as_gray=True)\n",
    "\n",
    "#edge_sobel = sobel(image)\n",
    "\n",
    "#fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "#axes[0].imshow(image, cmap=plt.cm.gray)\n",
    "#axes[0].set_title('Original')\n",
    "\n",
    "#axes[1].imshow(edge_sobel, cmap=plt.cm.gray)\n",
    "#axes[1].set_title('Sobel Edge Detection')\n",
    "\n",
    "#for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca389b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a65f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pe-process the date information\n",
    "df_train_dates = train_base[['cdate', 'total', 'X.sales']].copy()\n",
    "df_train_dates['cdate'] = df_train_dates['cdate'].astype('datetime64[ns]')\n",
    "df_train_dates = df_train_dates.sort_values(by='cdate').reset_index(drop=True)\n",
    "\n",
    "df_train_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da70fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dates = test_base[['cdate', 'X.sales']].copy()\n",
    "df_test_dates['cdate'] = df_test_dates['cdate'].astype('datetime64[ns]')\n",
    "df_test_dates = df_test_dates.sort_values(by='cdate').reset_index(drop=True)\n",
    "\n",
    "df_test_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "460db835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         month_sin  month_cos       day_sin  day_cos\n",
      "0    -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "1    -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "2    -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "3    -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "4    -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "...            ...        ...           ...      ...\n",
      "6909 -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "6910 -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "6911 -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "6912 -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "6913 -2.449294e-16        1.0 -2.449294e-16      1.0\n",
      "\n",
      "[6914 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "We want to preserve the cyclical nature of MM and D as features. One-hot encode year.\n",
    "'''\n",
    "df_train_dates = train_base[['cdate']].copy()\n",
    "df_train_dates['cdate'] = df_train_dates['cdate'].astype('datetime64[ns]')\n",
    "df_train_dates['month'] = pd.DatetimeIndex(df_train_dates['cdate']).month\n",
    "df_train_dates['day'] = pd.DatetimeIndex(df_train_dates['cdate']).day\n",
    "df_train_dates.head()\n",
    "\n",
    "cyclical = CyclicalTransformer(variables=None, drop_original=True)\n",
    "df_train_dates_new = cyclical.fit_transform(df_train_dates)\n",
    "df_train_dates_final=df_train_dates_new.drop(['cdate'], axis=1)\n",
    "print(df_train_dates_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13fe786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          cdate     month_sin  month_cos       day_sin  \\\n",
      "0 1970-01-01 00:00:00.000018039 -2.449294e-16        1.0 -2.449294e-16   \n",
      "1 1970-01-01 00:00:00.000018012 -2.449294e-16        1.0 -2.449294e-16   \n",
      "2 1970-01-01 00:00:00.000018262 -2.449294e-16        1.0 -2.449294e-16   \n",
      "3 1970-01-01 00:00:00.000018715 -2.449294e-16        1.0 -2.449294e-16   \n",
      "4 1970-01-01 00:00:00.000018012 -2.449294e-16        1.0 -2.449294e-16   \n",
      "\n",
      "   day_cos  \n",
      "0      1.0  \n",
      "1      1.0  \n",
      "2      1.0  \n",
      "3      1.0  \n",
      "4      1.0  \n"
     ]
    }
   ],
   "source": [
    "#Now do the same for the testing set:\n",
    "\n",
    "df_test_dates = test_base[['cdate']].copy()\n",
    "df_test_dates['cdate'] = df_test_dates['cdate'].astype('datetime64[ns]')\n",
    "df_test_dates['month'] = pd.DatetimeIndex(df_test_dates['cdate']).month\n",
    "df_test_dates['day'] = pd.DatetimeIndex(df_test_dates['cdate']).day\n",
    "df_test_dates.head()\n",
    "\n",
    "cyclical = CyclicalTransformer(variables=None, drop_original=True)\n",
    "df_test_dates_new = cyclical.fit_transform(df_test_dates)\n",
    "print(df_test_dates_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "87cef896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the the orignal data together with the cyclized dates and the text data\n",
    "train_concatenated_dates = pd.concat([train_base, df_train_dates_new], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dab814bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cdate                         cdate\n",
      "0     18012.0 1970-01-01 00:00:00.000018012\n",
      "1     18561.0 1970-01-01 00:00:00.000018561\n",
      "2     18012.0 1970-01-01 00:00:00.000018012\n",
      "3     18012.0 1970-01-01 00:00:00.000018012\n",
      "4     18012.0 1970-01-01 00:00:00.000018012\n",
      "...       ...                           ...\n",
      "6909  18593.0 1970-01-01 00:00:00.000018593\n",
      "6910  18012.0 1970-01-01 00:00:00.000018012\n",
      "6911  18024.0 1970-01-01 00:00:00.000018024\n",
      "6912  18790.0 1970-01-01 00:00:00.000018790\n",
      "6913  18544.0 1970-01-01 00:00:00.000018544\n",
      "\n",
      "[6914 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_concatenated = pd.concat([train_concatenated_dates, train_document_topic], axis=1)\n",
    "print(train_concatenated['cdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a5c04934",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concatenated_dates = pd.concat([test_base, df_test_dates_new], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0650468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concatenated = pd.concat([test_concatenated_dates, test_document_topic], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6b5fd9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       18012.0\n",
      "1       18561.0\n",
      "2       18012.0\n",
      "3       18012.0\n",
      "4       18012.0\n",
      "         ...   \n",
      "6909    18593.0\n",
      "6910    18012.0\n",
      "6911    18024.0\n",
      "6912    18790.0\n",
      "6913    18544.0\n",
      "Name: cdate, Length: 6914, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#remove extraneous cdate column\n",
    "train_concatenated = train_concatenated.loc[:,~train_concatenated.columns.duplicated()]\n",
    "print(train_concatenated['cdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "99a2d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concatenated = test_concatenated.loc[:,~test_concatenated.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0bea70e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    id  X.sales    cdate  \\\n",
      "0     a44a5f4c5e13910205404271e750e7bc      1.0  18012.0   \n",
      "1     5cd06ad38bc1e842b7f3e37210b8e574     42.0  18561.0   \n",
      "2     239eff19cbe449331801cd1e89c84d2c      3.0  18012.0   \n",
      "3     5034105b2e14a5d71522ed8054db89ac      4.0  18012.0   \n",
      "4     ab4d2c1023ad1b13f937988ea8cae5f4      1.0  18012.0   \n",
      "...                                ...      ...      ...   \n",
      "6909  b8b18cdc1f066586eb2a3c5326d2bb66      3.0  18593.0   \n",
      "6910  d3cc7ebd0a8001ede39dfebcc05cf9be      2.0  18012.0   \n",
      "6911  1912ac374a2e56327989f468cc4f09b9      2.0  18024.0   \n",
      "6912  22907775e92c304ccf77744d9c1989be      2.0  18790.0   \n",
      "6913  62defe67d57479ab0cd6d1ffb6525cbb      4.0  18544.0   \n",
      "\n",
      "                                            description    fee1    fee2  \\\n",
      "0     uSuuw tPc2s Oh5hm kbWyC oIv2i LmM6m TfoFy gnYR...   250.0   500.0   \n",
      "1                   zfG9T CWIL9 EPGsh RfQUu EDiBa hdBO7   250.0   500.0   \n",
      "2     UHWIX Csin3 giC2q uSw1I B58A7 F3KFu coELQ 7OFT...   750.0  1000.0   \n",
      "3     UFaHq GbOzi 5oiCZ 0dS4g 6FQLj WEBsK PjIBt gICf...   750.0  1000.0   \n",
      "4                                                  None   300.0   550.0   \n",
      "...                                                 ...     ...     ...   \n",
      "6909  XtC0N Csin3 p6h5k NDY1H MGQbb pQRRi EDiBa IRZp...   750.0  1000.0   \n",
      "6910  y9uu4 Csin3 giC2q 8gvmY B58A7 F3KFu coELQ 7OFT...   750.0  1000.0   \n",
      "6911  XAP1e p6h5k QgV2N UFmwP ZuHOA 77NPo VGPRq sgUN...     0.0   250.0   \n",
      "6912  wpksz QUff8 7ekHl q64Jx giC2q uxx1k QaOaK 0Fg9...   200.0   450.0   \n",
      "6913                                               None  1000.0  1250.0   \n",
      "\n",
      "       total  version_1  version_2  version_3  ...    Topic5    Topic6  \\\n",
      "0      0.055          0          0          1  ...  0.005556  0.005556   \n",
      "1     75.000          0          0          0  ...  0.016667  0.016667   \n",
      "2      0.158          0          0          1  ...  0.033333  0.033333   \n",
      "3      1.990          0          0          1  ...  0.004167  0.004167   \n",
      "4      1.000          0          0          1  ...  0.050000  0.050000   \n",
      "...      ...        ...        ...        ...  ...       ...       ...   \n",
      "6909   0.550          0          0          0  ...  0.159625  0.001961   \n",
      "6910   0.100          0          0          1  ...  0.033333  0.033333   \n",
      "6911   0.966          0          0          1  ...  0.003125  0.003125   \n",
      "6912   0.170          0          0          1  ...  0.010000  0.010000   \n",
      "6913   0.100          0          0          0  ...  0.050000  0.050000   \n",
      "\n",
      "        Topic7    Topic8    Topic9  height  width  channels  greyscale  \\\n",
      "0     0.005556  0.949996  0.005556    15.0   15.0       4.0        0.0   \n",
      "1     0.016667  0.849981  0.016667    21.0   15.0       3.0        0.0   \n",
      "2     0.033333  0.033333  0.033333     NaN    NaN       NaN        NaN   \n",
      "3     0.096422  0.004167  0.870244    15.0   15.0       0.0        1.0   \n",
      "4     0.050000  0.050000  0.050000    15.0   15.0       3.0        0.0   \n",
      "...        ...       ...       ...     ...    ...       ...        ...   \n",
      "6909  0.771827  0.001961  0.001961     NaN    NaN       NaN        NaN   \n",
      "6910  0.033333  0.033333  0.033333     NaN    NaN       NaN        NaN   \n",
      "6911  0.003125  0.971875  0.003125    14.0   15.0       4.0        0.0   \n",
      "6912  0.010000  0.909998  0.010000    15.0   15.0       4.0        0.0   \n",
      "6913  0.050000  0.050000  0.050000    23.0   15.0       4.0        0.0   \n",
      "\n",
      "      large_image  \n",
      "0             0.0  \n",
      "1             0.0  \n",
      "2             NaN  \n",
      "3             0.0  \n",
      "4             0.0  \n",
      "...           ...  \n",
      "6909          NaN  \n",
      "6910          NaN  \n",
      "6911          0.0  \n",
      "6912          0.0  \n",
      "6913          0.0  \n",
      "\n",
      "[6914 rows x 610 columns]\n"
     ]
    }
   ],
   "source": [
    "train_merged= train_concatenated.merge(image_df, how='left', on='id')\n",
    "print(train_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d32b77bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X.sales    cdate    fee1    fee2   total  version_1  version_2  \\\n",
      "0         1.0  18012.0   250.0   500.0   0.055          0          0   \n",
      "1        42.0  18561.0   250.0   500.0  75.000          0          0   \n",
      "2         3.0  18012.0   750.0  1000.0   0.158          0          0   \n",
      "3         4.0  18012.0   750.0  1000.0   1.990          0          0   \n",
      "4         1.0  18012.0   300.0   550.0   1.000          0          0   \n",
      "...       ...      ...     ...     ...     ...        ...        ...   \n",
      "6909      3.0  18593.0   750.0  1000.0   0.550          0          0   \n",
      "6910      2.0  18012.0   750.0  1000.0   0.100          0          0   \n",
      "6911      2.0  18024.0     0.0   250.0   0.966          0          0   \n",
      "6912      2.0  18790.0   200.0   450.0   0.170          0          0   \n",
      "6913      4.0  18544.0  1000.0  1250.0   0.100          0          0   \n",
      "\n",
      "      version_3  version_4  version_None  ...    Topic2    Topic3    Topic4  \\\n",
      "0             1          0             0  ...  0.005558  0.005556  0.005556   \n",
      "1             0          0             1  ...  0.016667  0.016685  0.016667   \n",
      "2             1          0             0  ...  0.033333  0.033333  0.033333   \n",
      "3             1          0             0  ...  0.004167  0.004167  0.004167   \n",
      "4             1          0             0  ...  0.050000  0.550000  0.050000   \n",
      "...         ...        ...           ...  ...       ...       ...       ...   \n",
      "6909          0          1             0  ...  0.001961  0.001961  0.001961   \n",
      "6910          1          0             0  ...  0.033333  0.033333  0.033344   \n",
      "6911          1          0             0  ...  0.003125  0.003125  0.003125   \n",
      "6912          1          0             0  ...  0.010000  0.010000  0.010000   \n",
      "6913          0          0             1  ...  0.050000  0.550000  0.050000   \n",
      "\n",
      "        Topic5    Topic6    Topic7    Topic8    Topic9  greyscale  large_image  \n",
      "0     0.005556  0.005556  0.005556  0.949996  0.005556        0.0          0.0  \n",
      "1     0.016667  0.016667  0.016667  0.849981  0.016667        0.0          0.0  \n",
      "2     0.033333  0.033333  0.033333  0.033333  0.033333        NaN          NaN  \n",
      "3     0.004167  0.004167  0.096422  0.004167  0.870244        1.0          0.0  \n",
      "4     0.050000  0.050000  0.050000  0.050000  0.050000        0.0          0.0  \n",
      "...        ...       ...       ...       ...       ...        ...          ...  \n",
      "6909  0.159625  0.001961  0.771827  0.001961  0.001961        NaN          NaN  \n",
      "6910  0.033333  0.033333  0.033333  0.033333  0.033333        NaN          NaN  \n",
      "6911  0.003125  0.003125  0.003125  0.971875  0.003125        0.0          0.0  \n",
      "6912  0.010000  0.010000  0.010000  0.909998  0.010000        0.0          0.0  \n",
      "6913  0.050000  0.050000  0.050000  0.050000  0.050000        0.0          0.0  \n",
      "\n",
      "[6914 rows x 605 columns]\n"
     ]
    }
   ],
   "source": [
    "#Drop the less redundant image variables and remove 'description' and 'id'\n",
    "train_merged = train_merged.drop(['id', 'description', 'channels', 'height', 'width'], axis=1)\n",
    "print(train_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "17743c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X.sales    cdate    fee1    fee2   total  version_1  version_2  \\\n",
      "0         1.0  18012.0   250.0   500.0   0.055          0          0   \n",
      "1        42.0  18561.0   250.0   500.0  75.000          0          0   \n",
      "2         3.0  18012.0   750.0  1000.0   0.158          0          0   \n",
      "3         4.0  18012.0   750.0  1000.0   1.990          0          0   \n",
      "4         1.0  18012.0   300.0   550.0   1.000          0          0   \n",
      "...       ...      ...     ...     ...     ...        ...        ...   \n",
      "6909      3.0  18593.0   750.0  1000.0   0.550          0          0   \n",
      "6910      2.0  18012.0   750.0  1000.0   0.100          0          0   \n",
      "6911      2.0  18024.0     0.0   250.0   0.966          0          0   \n",
      "6912      2.0  18790.0   200.0   450.0   0.170          0          0   \n",
      "6913      4.0  18544.0  1000.0  1250.0   0.100          0          0   \n",
      "\n",
      "      version_3  version_4  version_None  ...    Topic2    Topic3    Topic4  \\\n",
      "0             1          0             0  ...  0.005558  0.005556  0.005556   \n",
      "1             0          0             1  ...  0.016667  0.016685  0.016667   \n",
      "2             1          0             0  ...  0.033333  0.033333  0.033333   \n",
      "3             1          0             0  ...  0.004167  0.004167  0.004167   \n",
      "4             1          0             0  ...  0.050000  0.550000  0.050000   \n",
      "...         ...        ...           ...  ...       ...       ...       ...   \n",
      "6909          0          1             0  ...  0.001961  0.001961  0.001961   \n",
      "6910          1          0             0  ...  0.033333  0.033333  0.033344   \n",
      "6911          1          0             0  ...  0.003125  0.003125  0.003125   \n",
      "6912          1          0             0  ...  0.010000  0.010000  0.010000   \n",
      "6913          0          0             1  ...  0.050000  0.550000  0.050000   \n",
      "\n",
      "        Topic5    Topic6    Topic7    Topic8    Topic9  greyscale  large_image  \n",
      "0     0.005556  0.005556  0.005556  0.949996  0.005556        0.0          0.0  \n",
      "1     0.016667  0.016667  0.016667  0.849981  0.016667        0.0          0.0  \n",
      "2     0.033333  0.033333  0.033333  0.033333  0.033333        NaN          NaN  \n",
      "3     0.004167  0.004167  0.096422  0.004167  0.870244        1.0          0.0  \n",
      "4     0.050000  0.050000  0.050000  0.050000  0.050000        0.0          0.0  \n",
      "...        ...       ...       ...       ...       ...        ...          ...  \n",
      "6909  0.159625  0.001961  0.771827  0.001961  0.001961        NaN          NaN  \n",
      "6910  0.033333  0.033333  0.033333  0.033333  0.033333        NaN          NaN  \n",
      "6911  0.003125  0.003125  0.003125  0.971875  0.003125        0.0          0.0  \n",
      "6912  0.010000  0.010000  0.010000  0.909998  0.010000        0.0          0.0  \n",
      "6913  0.050000  0.050000  0.050000  0.050000  0.050000        0.0          0.0  \n",
      "\n",
      "[6914 rows x 605 columns]\n"
     ]
    }
   ],
   "source": [
    "test_merged= test_concatenated.merge(image_df, how='left', on='id')\n",
    "print(train_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b17679b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X.sales    cdate    fee1    fee2   total  version_1  version_2  \\\n",
      "0         1.0  18012.0   250.0   500.0   0.055          0          0   \n",
      "1        42.0  18561.0   250.0   500.0  75.000          0          0   \n",
      "2         3.0  18012.0   750.0  1000.0   0.158          0          0   \n",
      "3         4.0  18012.0   750.0  1000.0   1.990          0          0   \n",
      "4         1.0  18012.0   300.0   550.0   1.000          0          0   \n",
      "...       ...      ...     ...     ...     ...        ...        ...   \n",
      "6909      3.0  18593.0   750.0  1000.0   0.550          0          0   \n",
      "6910      2.0  18012.0   750.0  1000.0   0.100          0          0   \n",
      "6911      2.0  18024.0     0.0   250.0   0.966          0          0   \n",
      "6912      2.0  18790.0   200.0   450.0   0.170          0          0   \n",
      "6913      4.0  18544.0  1000.0  1250.0   0.100          0          0   \n",
      "\n",
      "      version_3  version_4  version_None  ...    Topic2    Topic3    Topic4  \\\n",
      "0             1          0             0  ...  0.005558  0.005556  0.005556   \n",
      "1             0          0             1  ...  0.016667  0.016685  0.016667   \n",
      "2             1          0             0  ...  0.033333  0.033333  0.033333   \n",
      "3             1          0             0  ...  0.004167  0.004167  0.004167   \n",
      "4             1          0             0  ...  0.050000  0.550000  0.050000   \n",
      "...         ...        ...           ...  ...       ...       ...       ...   \n",
      "6909          0          1             0  ...  0.001961  0.001961  0.001961   \n",
      "6910          1          0             0  ...  0.033333  0.033333  0.033344   \n",
      "6911          1          0             0  ...  0.003125  0.003125  0.003125   \n",
      "6912          1          0             0  ...  0.010000  0.010000  0.010000   \n",
      "6913          0          0             1  ...  0.050000  0.550000  0.050000   \n",
      "\n",
      "        Topic5    Topic6    Topic7    Topic8    Topic9  greyscale  large_image  \n",
      "0     0.005556  0.005556  0.005556  0.949996  0.005556        0.0          0.0  \n",
      "1     0.016667  0.016667  0.016667  0.849981  0.016667        0.0          0.0  \n",
      "2     0.033333  0.033333  0.033333  0.033333  0.033333        NaN          NaN  \n",
      "3     0.004167  0.004167  0.096422  0.004167  0.870244        1.0          0.0  \n",
      "4     0.050000  0.050000  0.050000  0.050000  0.050000        0.0          0.0  \n",
      "...        ...       ...       ...       ...       ...        ...          ...  \n",
      "6909  0.159625  0.001961  0.771827  0.001961  0.001961        NaN          NaN  \n",
      "6910  0.033333  0.033333  0.033333  0.033333  0.033333        NaN          NaN  \n",
      "6911  0.003125  0.003125  0.003125  0.971875  0.003125        0.0          0.0  \n",
      "6912  0.010000  0.010000  0.010000  0.909998  0.010000        0.0          0.0  \n",
      "6913  0.050000  0.050000  0.050000  0.050000  0.050000        0.0          0.0  \n",
      "\n",
      "[6914 rows x 605 columns]\n"
     ]
    }
   ],
   "source": [
    "test_merged = test_merged.drop(['id', 'description', 'channels','height','width'], axis=1)\n",
    "print(train_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "69fa1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Nan's to zero\n",
    "\n",
    "#train_merged['height'] = train_merged['height'].fillna(train_merged['height'].mean(), inplace=True)\n",
    "#train_merged['width']=train_merged['width'].fillna(train_merged['width'].median(), inplace=True)\n",
    "train_merged['greyscale']=train_merged['greyscale'].fillna(0)\n",
    "train_merged['large_image'] = train_merged['large_image'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8599bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X.sales    cdate    fee1    fee2   total  version_1  version_2  \\\n",
      "0         1.0  18012.0   250.0   500.0   0.055          0          0   \n",
      "1        42.0  18561.0   250.0   500.0  75.000          0          0   \n",
      "2         3.0  18012.0   750.0  1000.0   0.158          0          0   \n",
      "3         4.0  18012.0   750.0  1000.0   1.990          0          0   \n",
      "4         1.0  18012.0   300.0   550.0   1.000          0          0   \n",
      "...       ...      ...     ...     ...     ...        ...        ...   \n",
      "6909      3.0  18593.0   750.0  1000.0   0.550          0          0   \n",
      "6910      2.0  18012.0   750.0  1000.0   0.100          0          0   \n",
      "6911      2.0  18024.0     0.0   250.0   0.966          0          0   \n",
      "6912      2.0  18790.0   200.0   450.0   0.170          0          0   \n",
      "6913      4.0  18544.0  1000.0  1250.0   0.100          0          0   \n",
      "\n",
      "      version_3  version_4  version_None  ...    Topic2    Topic3    Topic4  \\\n",
      "0             1          0             0  ...  0.005558  0.005556  0.005556   \n",
      "1             0          0             1  ...  0.016667  0.016685  0.016667   \n",
      "2             1          0             0  ...  0.033333  0.033333  0.033333   \n",
      "3             1          0             0  ...  0.004167  0.004167  0.004167   \n",
      "4             1          0             0  ...  0.050000  0.550000  0.050000   \n",
      "...         ...        ...           ...  ...       ...       ...       ...   \n",
      "6909          0          1             0  ...  0.001961  0.001961  0.001961   \n",
      "6910          1          0             0  ...  0.033333  0.033333  0.033344   \n",
      "6911          1          0             0  ...  0.003125  0.003125  0.003125   \n",
      "6912          1          0             0  ...  0.010000  0.010000  0.010000   \n",
      "6913          0          0             1  ...  0.050000  0.550000  0.050000   \n",
      "\n",
      "        Topic5    Topic6    Topic7    Topic8    Topic9  greyscale  large_image  \n",
      "0     0.005556  0.005556  0.005556  0.949996  0.005556        0.0          0.0  \n",
      "1     0.016667  0.016667  0.016667  0.849981  0.016667        0.0          0.0  \n",
      "2     0.033333  0.033333  0.033333  0.033333  0.033333        0.0          0.0  \n",
      "3     0.004167  0.004167  0.096422  0.004167  0.870244        1.0          0.0  \n",
      "4     0.050000  0.050000  0.050000  0.050000  0.050000        0.0          0.0  \n",
      "...        ...       ...       ...       ...       ...        ...          ...  \n",
      "6909  0.159625  0.001961  0.771827  0.001961  0.001961        0.0          0.0  \n",
      "6910  0.033333  0.033333  0.033333  0.033333  0.033333        0.0          0.0  \n",
      "6911  0.003125  0.003125  0.003125  0.971875  0.003125        0.0          0.0  \n",
      "6912  0.010000  0.010000  0.010000  0.909998  0.010000        0.0          0.0  \n",
      "6913  0.050000  0.050000  0.050000  0.050000  0.050000        0.0          0.0  \n",
      "\n",
      "[6914 rows x 605 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fac9d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged['greyscale']=test_merged['greyscale'].fillna(0)\n",
    "test_merged['large_image'] = test_merged['large_image'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8da05f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling: Linear Regression and SGD\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5c701318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = train_merged['total'].copy()\n",
    "X = train_merged.drop(['total'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7e8d4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3b5e56e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OLS = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a9913a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OLS_predictions = OLS.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f36ff295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26442839.690172672"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, OLS_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b2d71b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# standardize input\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ceca11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = SGDRegressor(loss = 'huber', alpha = 0.000001, epsilon = 0).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8d58b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_predictions = SGD.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "81a9b779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.338762914302487"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mean_absolute_error(y_test, SGD_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "11fd19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {'alpha':[0.1,0.01, 0],\n",
    "              'learning_rate':['adaptive'],\n",
    "              'max_iter':[1000, 2000],\n",
    "              'penalty':['l2','l1','elasticnet'],\n",
    "              'loss':['huber', 'epsilon_insensitive','squared_epsilon_insensitive', 'squared_error'],\n",
    "              'eta0': [0.01, 1, 10, 20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "80618dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_ = SGDRegressor(random_state = 1)\n",
    "g_search = GridSearchCV(estimator = sgd_, param_grid = param_grid, cv = 3, n_jobs = 1, verbose = 2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdbb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   2.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  11.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  15.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  13.9s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  14.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   2.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   3.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   9.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  25.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  21.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  23.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  35.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  13.9s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  15.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   4.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  14.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  11.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  12.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  11.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  12.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  13.5s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   5.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   4.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   6.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  31.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  26.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  28.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  13.2s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  15.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  17.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   4.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   1.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  20.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  12.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  13.3s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  10.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   3.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  11.1s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   4.6s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   1.4s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  23.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  26.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  20.8s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   9.7s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   4.9s\n",
      "[CV] END alpha=0.1, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  13.1s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   3.1s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   2.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  10.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  10.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  11.1s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   4.7s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   5.9s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   5.8s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   3.4s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   2.1s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  20.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  22.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  23.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   4.9s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   7.1s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   6.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   3.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   2.0s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  12.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  11.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   7.4s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   7.0s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   6.9s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   3.9s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   2.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  22.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  25.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  26.3s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   7.5s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   7.6s\n",
      "[CV] END alpha=0.1, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   6.2s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.7s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   6.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  14.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  12.6s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  13.3s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  19.4s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  16.3s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   7.2s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   7.2s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  21.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  22.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  23.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  15.5s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  19.1s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  13.4s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   2.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   1.5s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  15.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  13.4s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   4.6s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   4.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   5.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   2.8s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   1.6s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  24.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  29.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  26.0s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   4.6s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   4.2s\n",
      "[CV] END alpha=0.1, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   5.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  11.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  12.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  12.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  13.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  13.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  12.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  11.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  23.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  35.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  39.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  27.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  28.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  28.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  13.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  14.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  15.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  11.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  22.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  25.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  26.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  30.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  28.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  33.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   8.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  14.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  14.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  13.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  10.9s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  10.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  27.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  34.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  37.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  34.1s\n",
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  16.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  30.3s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.6s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   7.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  17.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  21.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  27.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  21.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  12.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  11.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  13.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  16.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  29.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  30.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  33.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  37.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  42.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  34.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  12.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  13.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  12.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  12.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  10.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  10.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  21.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  22.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  32.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  27.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  26.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=10, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  31.1s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  15.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  16.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  23.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  19.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  18.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  21.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  15.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  16.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  13.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  29.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  35.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  41.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  38.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  34.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  37.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   8.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   7.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   7.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  14.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  13.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  15.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  16.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  17.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  18.0s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  19.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  17.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  26.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  27.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  23.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  29.0s\n",
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  31.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.01, eta0=20, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  38.3s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   2.0s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   2.0s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.9s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.7s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   2.8s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   1.1s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   1.8s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   1.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   1.9s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   2.0s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.7s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   0.8s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  10.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  11.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  12.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  17.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  12.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  32.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  30.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  31.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  27.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  25.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  26.1s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=   0.6s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=   0.7s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   0.6s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   0.3s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=   0.2s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=0.01, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.1s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.4s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=   1.3s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.8s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.3s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l2; total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  13.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=l1; total time=  16.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  16.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  13.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=1000, penalty=elasticnet; total time=  12.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  11.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l2; total time=  11.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  21.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  22.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=l1; total time=  22.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  21.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  21.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_epsilon_insensitive, max_iter=2000, penalty=elasticnet; total time=  20.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l2; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  11.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=l1; total time=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  10.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  10.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=1000, penalty=elasticnet; total time=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  12.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l2; total time=  10.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  20.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  20.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=l1; total time=  21.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  20.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  23.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jamie/venv/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0, eta0=1, learning_rate=adaptive, loss=squared_error, max_iter=2000, penalty=elasticnet; total time=  21.8s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l2; total time=   0.4s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.1s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=1000, penalty=elasticnet; total time=   1.2s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.6s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   1.0s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=l1; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=huber, max_iter=2000, penalty=elasticnet; total time=   0.9s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l2; total time=   0.5s\n",
      "[CV] END alpha=0, eta0=10, learning_rate=adaptive, loss=epsilon_insensitive, max_iter=1000, penalty=l1; total time=   1.4s\n"
     ]
    }
   ],
   "source": [
    "g_search.fit(x_train, y_train);\n",
    "print(g_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "870a108f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.133432070749297"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD = SGDRegressor(alpha=0, eta0=10, learning_rate='adaptive', loss='epsilon_insensitive', max_iter=1000, penalty='l2').fit(x_train, y_train)\n",
    "SGD_predictions= SGD.predict(x_test)\n",
    "mean_absolute_error(y_test, SGD_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "31f6c138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.65868557437726"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD1 = SGDRegressor(alpha=0.0001, eta0=0.01, learning_rate='adaptive', loss='squared_epsilon_insensitive', max_iter=9000, penalty='elasticnet').fit(x_train, y_train)\n",
    "SGD_predictions1= SGD1.predict(x_test)\n",
    "mean_absolute_error(y_test, SGD_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "afd16b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21492509 0.19555863 0.03450203 ... 0.21941478 0.1928126  0.10703348]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "930d6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = pd.read_csv('/Users/Jamie/venv/pred.csv')\n",
    "#SGD_predictions.to_csv('data/nft_img_pred_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9b264852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize input\n",
    "scaler = StandardScaler()\n",
    "#x_train = scaler.fit_transform(x_train)\n",
    "final_SGD_test = scaler.fit_transform(test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5daa3fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nft_img_pred_1 = pred.copy()\n",
    "nft_img_pred_1['total'].median()\n",
    "#nft_img_pred_1['total'].fillna(nft_img_pred_1['fee1'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "762b5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "nft_img_pred_1[nft_img_pred_1['total'] < 0] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2aa4c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "nft_img_pred_1['total'] = SGD.predict(final_SGD_test)\n",
    "nft_img_pred_1.to_csv('/Users/Jamie/venv/nft_img_pred_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34bed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
